{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This practice implies testing logic laws (commutation, association, transition) in texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import tqdm\n",
    "warnings.filterwarnings('ignore')\n",
    "from gensim.models import Word2Vec\n",
    "from string import punctuation\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from datetime import datetime\n",
    "exclude = set(punctuation + '0123456789[]—«»–')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For fitting there will be texts acquired and saved from kaggle. Standard preprocessing and spitting will be applied to all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256980 sentences loaded\n"
     ]
    }
   ],
   "source": [
    "with open('testData.tsv', 'r', encoding='utf-8') as f:\n",
    "    text1 = f.read()\n",
    "text1 = text1.replace('\\n', ' ')\n",
    "sents = sent_tokenize(text1)\n",
    "for s in sents:\n",
    "    buf = ''.join(ch for ch in s if ch not in exclude).lower()\n",
    "    sentences.append(word_tokenize(buf))\n",
    "print(\"%s sentences loaded\" % len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "777122 sentences loaded\n"
     ]
    }
   ],
   "source": [
    "with open('unlabeledTrainData.tsv', 'r', encoding='utf-8') as f:\n",
    "    text2 = f.read()\n",
    "text2 = text2.replace('\\n', ' ')\n",
    "sents = sent_tokenize(text2)\n",
    "for s in sents:\n",
    "    buf = ''.join(ch for ch in s if ch not in exclude).lower()\n",
    "    sentences.append(word_tokenize(buf))\n",
    "print(\"%s sentences loaded\" % len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:06:44.004894 elapsed\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.now()\n",
    "model = Word2Vec(sentences, min_count=1)\n",
    "print('%s elapsed' % (datetime.now() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.save('kaggleDataModel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Word2Vec.load('kaggleDataModel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lexically close words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar to:  man\n",
      "[('woman', 0.7610129714012146), ('guy', 0.7300944328308105), ('soldier', 0.7272496819496155), ('person', 0.7112268805503845), ('boy', 0.6943151354789734), ('lady', 0.6704555153846741), ('lad', 0.6570690274238586), ('monk', 0.6562662720680237), ('gentleman', 0.649365246295929), ('doctor', 0.6390960812568665)]\n",
      "\n",
      "Similar to:  woman\n",
      "[('girl', 0.8390386700630188), ('lady', 0.7814967036247253), ('man', 0.7610130906105042), ('prostitute', 0.760490357875824), ('widow', 0.7088424563407898), ('person', 0.6998924612998962), ('teenager', 0.6989359855651855), ('nurse', 0.6923184394836426), ('housewife', 0.6889249682426453), ('lad', 0.6849828958511353)]\n",
      "\n",
      "Similar to:  run\n",
      "[('drive', 0.6993310451507568), ('walk', 0.6978382468223572), ('wander', 0.6943727135658264), ('go', 0.6800222396850586), ('move', 0.6617880463600159), ('fly', 0.6617198586463928), ('running', 0.6562157273292542), ('slip', 0.6523092985153198), ('bump', 0.6421452760696411), ('haul', 0.6290749907493591)]\n",
      "\n",
      "Similar to:  sleep\n",
      "[('bed', 0.666144609451294), ('drink', 0.6292545199394226), ('burn', 0.5836717486381531), ('stop', 0.5664850473403931), ('chair', 0.5661994814872742), ('eat', 0.5652629733085632), ('wash', 0.561310350894928), ('shoot', 0.5484833121299744), ('leave', 0.544562816619873), ('lunch', 0.5425955653190613)]\n",
      "\n",
      "Similar to:  love\n",
      "[('bermuda', 0.6493183970451355), ('hate', 0.5986714363098145), ('loved', 0.5135771632194519), ('comingofagelove', 0.4933352470397949), ('loves', 0.491583913564682), ('adore', 0.490633100271225), ('passion', 0.48988139629364014), ('romantic', 0.4840010106563568), ('rawhead', 0.4818587303161621), ('dislike', 0.47886693477630615)]\n",
      "\n",
      "Similar to:  peace\n",
      "[('freedom', 0.714144766330719), ('mankind', 0.7016671895980835), ('humanity', 0.6699041724205017), ('happiness', 0.6599718332290649), ('civilization', 0.6577023863792419), ('afghanistan', 0.6495787501335144), ('democracy', 0.632415235042572), ('forgiveness', 0.6280192732810974), ('sacrifice', 0.6247733235359192), ('communism', 0.6231562495231628)]\n",
      "\n",
      "Similar to:  good\n",
      "[('decent', 0.7893454432487488), ('great', 0.7562857270240784), ('bad', 0.7433226108551025), ('fine', 0.7036449313163757), ('nice', 0.6747317314147949), ('cool', 0.6680628061294556), ('solid', 0.6425601840019226), ('terrific', 0.634570837020874), ('passable', 0.626611053943634), ('fantastic', 0.6109205484390259)]\n",
      "\n",
      "Similar to:  bad\n",
      "[('terrible', 0.7622318863868713), ('horrible', 0.7465754747390747), ('good', 0.7433227300643921), ('stupid', 0.7178111672401428), ('lame', 0.6965764164924622), ('awful', 0.6913819313049316), ('lousy', 0.6876482367515564), ('crappy', 0.685447633266449), ('poor', 0.6521151661872864), ('dumb', 0.646263599395752)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "words = ('man', 'woman', 'run', 'sleep', 'love', 'peace', 'good', 'bad')\n",
    "for word in words:\n",
    "    print('Similar to: ', word)\n",
    "    print(model.most_similar(word))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, words were found correctly. Out of 10 words, which are marked as lexically close, there are some that fall out of scope in each case. it's related to text context, which was used for fitting, and also with using idioms. Many words, which are characterized as very close (> 0.7) we can call synonims, e.g. bad -> terrible, horrible; good -> descent; woman -> lady."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Association rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('mother', 0.86660236120224), ('husband', 0.8353188037872314), ('daughter', 0.8319278955459595)]\n",
      "[('wellcompressed', 0.5845099091529846), ('anythinggood', 0.5755310654640198), ('boycotting', 0.569663405418396)]\n",
      "[('goodlooking', 0.6495766043663025), ('tough', 0.6393650770187378), ('handsome', 0.6369926929473877)]\n",
      "[('woman', 0.7167847752571106), ('soldier', 0.6674887537956238), ('guy', 0.6461684107780457)]\n",
      "[('lady', 0.6756771206855774), ('widow', 0.6608632206916809), ('prostitute', 0.6565677523612976)]\n",
      "[('worst', 0.8059595227241516), ('funniest', 0.6849568486213684), ('finest', 0.6358859539031982)]\n"
     ]
    }
   ],
   "source": [
    "# According to example at https://rare-technologies.com/deep-learning-with-word2vec-and-gensim/\n",
    "# \"boy\" is to \"father\" as \"girl\" is to ...?\n",
    "# model.most_similar(['girl', 'father'], ['boy'], topn=3)\n",
    "relations = (\n",
    "    (['girl', 'father'], ['boy']),\n",
    "    (['hater', 'love'], ['lover']),\n",
    "    (['smart', 'dumb'], ['stupid']),\n",
    "    (['wife', 'man'], ['husband']),\n",
    "    (['woman', 'girl'], ['guy']),\n",
    "    (['bad', 'best'], ['good']),\n",
    ")\n",
    "for i, j in relations:\n",
    "    print(model.most_similar(i, j, topn=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Association for cases 1, 4, 6 were found correctly. For case 3 association is found merely correct, the word hase close meaning. \n",
    "For case 5 there was found a word that is simply close by meaning but doesn't comply associative rule. The worst case is 2, word is not found at all. This may be because of the fitted context, which doesn't have enough examples for reproducing such complicated associative rules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HUSBAND is extra\n",
      "CEREAL is extra\n",
      "PLEASURE is extra\n",
      "DUMB is extra\n",
      "SKI is extra\n",
      "CAR is extra\n",
      "HIGH is extra\n",
      "BROTHER is extra\n"
     ]
    }
   ],
   "source": [
    "extra_words = (\n",
    "    \"man guy husband girl\",\n",
    "    \"breakfast cereal dinner lunch\",\n",
    "    \"good great pleasure bad\",\n",
    "    \"smart clever sophisticated dumb\",\n",
    "    \"run climb ski lay\",\n",
    "    \"table chair car\",\n",
    "    \"green yellow high black\",\n",
    "    \"mother sister daughter brother\"\n",
    ")\n",
    "for sentence in extra_words:\n",
    "    print(\"{} is extra\".format(model.doesnt_match(sentence.split()).upper()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extra words are found absolutely correctly for lines 2, 3, 4, 6, 7, 8. For other lines extra words can be found differently from different points of views, e.g. line 1 has word 'husband' chosen, which is a married man, and other words are simply too abstract. From another point of view, we can pick word 'girl' as extra, because it's a feminine, while other words are masculine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commutation rule doesn't apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "father => mother, mother != father\n",
      "mother => ['daughter', 'grandmother', 'aunt']\n",
      "\n",
      "high => highest, highest != high\n",
      "highest => ['lowest', 'biggest', 'sole']\n",
      "\n",
      "dumb => smart, smart != dumb\n",
      "smart => ['intelligent', 'brave', 'charming']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# \"a b\" => \"b a\" , т.е. 'a' относится к 'b', как 'b' к 'a'\n",
    "words_commutations = (\n",
    "    \"father mother\",\n",
    "    \"high highest\",\n",
    "    \"dumb smart\",\n",
    ")\n",
    "for words in words_commutations:\n",
    "    a, b = words.split()\n",
    "    top3 = model.most_similar([b, b], [a], topn=3)\n",
    "    print('{a} => {b}, as {b} to {a}'.format(a=a, b=b) \n",
    "          if (a in top3) else '{a} => {b}, {b} != {a}'.format(a=a, b=b))\n",
    "    print(\"{b} => {c}\".format(b=b, c=[word for word, prob in top3]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transitive rule doesn't apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "father => day, day => ['morning', 'week', 'night']\n",
      "father => ['mother', 'son', 'husband']\n",
      "Транзитивность не выполняется\n",
      "\n",
      "lunch => bigger, bigger => ['larger', 'smaller', 'stronger']\n",
      "lunch => ['meal', 'coffee', 'breakfast']\n",
      "Транзитивность не выполняется\n",
      "\n",
      "going => sun, sun => ['river', 'tower', 'sky']\n",
      "going => ['supposed', 'ready', 'trying']\n",
      "Транзитивность не выполняется\n",
      "\n"
     ]
    }
   ],
   "source": [
    "words_transitives = (\n",
    "    \"father day\", \n",
    "    \"lunch bigger\", \n",
    "    \"going sun\"\n",
    ")\n",
    "for words in words_transitives:\n",
    "    a, b = words.split()\n",
    "    predicted = model.most_similar([b, b], [a], topn=3)\n",
    "    print(\"{a} => {b}, {b} => {c}\".format(a=a, b=b, c=[word for word, prob in predicted]))\n",
    "    trans = model.most_similar([a, a], [a], topn=3)\n",
    "    print(\"{a} => {c}\".format(a=a, c=[word for word, prob in trans]))\n",
    "    is_transitive = bool(set([word for word, prob in predicted]) & set([word for word, prob in trans]))\n",
    "    print('Транзитивность соблюдается' if is_transitive else 'Транзитивность не выполняется')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transitive rule applies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he => his, his => ['sams', 'jacks', 'bens']\n",
      "he => ['her', 'jacks', 'sams']\n",
      "Транзитивность выполняется\n",
      "\n",
      "big => bigger, bigger => ['larger', 'quicker', 'scarier']\n",
      "big => ['smaller', 'larger', 'cheaper']\n",
      "Транзитивность выполняется\n",
      "\n",
      "going => went, went => ['came', 'ran', 'took']\n",
      "going => ['came', 'goes', 'ran']\n",
      "Транзитивность выполняется\n",
      "\n"
     ]
    }
   ],
   "source": [
    "words_transitives = (\n",
    "    \"he his\", \n",
    "    \"big bigger\", \n",
    "    \"going went\"\n",
    ")\n",
    "for words in words_transitives:\n",
    "    a, b = words.split()\n",
    "    predicted = model.most_similar([b, b], [a], topn=3)\n",
    "    print(\"{a} => {b}, {b} => {c}\".format(a=a, b=b, c=[word for word, prob in predicted]))\n",
    "    trans = model.most_similar([a, b], [a], topn=3)\n",
    "    print(\"{a} => {c}\".format(a=a, c=[word for word, prob in trans]))\n",
    "    is_transitive = bool(set([word for word, prob in predicted]) & set([word for word, prob in trans]))\n",
    "    print('Транзитивность выполняется' if is_transitive else 'Транзитивность не выполняется')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "During this practice there was constructed a word2vec model with 777 thousand sentences. Completed tasks are:\n",
    "- Close by meaning words found\n",
    "- Association rule reproduced\n",
    "- Extra words found from each set\n",
    "- Examples of transitive and commutative rules applications and opposite.\n",
    "<br><br>\n",
    "The model could have been build more generalized and precise, if we had a bigger set of sentences for learning, but as it turns out to be - it can take a lot of computational resources."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
