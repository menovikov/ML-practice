{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis testing\n",
    "\n",
    "The hypothesis, which is going to be tested, states that different from lexical perspective styles of writing have equal distribution of parts of speech. \n",
    "<br><br>\n",
    "Styles to test:\n",
    "- science\n",
    "- blogs\n",
    "\n",
    "Both corresponding files contain collected from the internet articles, such as geographical and geological articles from open e-libraries and also articles from Leonid Kaganov's blog, Drogoy Journal, etc. All articles are grouped in 2 files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk import FreqDist \n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "from string import punctuation\n",
    "exclude = set(punctuation + '0123456789[]—«»–')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Science texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('science.txt', 'r', encoding='utf-8') as t:\n",
    "    text = t.read().replace('-\\n', '')\n",
    "    buf = ''.join(ch for ch in text if ch not in exclude)\n",
    "    science_tokens = WhitespaceTokenizer().tokenize(buf.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Токенов:  18623\n",
      "Типов:  6565\n"
     ]
    }
   ],
   "source": [
    "print('Токенов: ', len(science_tokens))\n",
    "print('Типов: ', len(set(science_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "morph = MorphAnalyzer()\n",
    "freq = FreqDist(science_tokens)\n",
    "science_parts_of_speech = {}\n",
    "for word in list(set(science_tokens)):\n",
    "    part_of_speech = str(morph.parse(word)[0].tag)[:4]\n",
    "    if part_of_speech in science_parts_of_speech.keys():\n",
    "        science_parts_of_speech[part_of_speech] += int(freq[word])\n",
    "    else:\n",
    "        science_parts_of_speech[part_of_speech] = int(freq[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ADJF': 3246,\n",
       " 'ADJS': 144,\n",
       " 'ADVB': 485,\n",
       " 'COMP': 26,\n",
       " 'CONJ': 1438,\n",
       " 'GRND': 62,\n",
       " 'INFN': 274,\n",
       " 'INTJ': 52,\n",
       " 'LATN': 196,\n",
       " 'NOUN': 7832,\n",
       " 'NPRO': 274,\n",
       " 'NUMR': 45,\n",
       " 'PNCT': 12,\n",
       " 'PRCL': 270,\n",
       " 'PRED': 40,\n",
       " 'PREP': 2134,\n",
       " 'PRTF': 414,\n",
       " 'PRTS': 150,\n",
       " 'ROMN': 28,\n",
       " 'UNKN': 463,\n",
       " 'VERB': 1038}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "science_parts_of_speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('blogs.txt', 'r', encoding='utf-8') as t:\n",
    "    text = t.read().replace('-\\n', '')\n",
    "    buf = ''.join(ch for ch in text if ch not in exclude)\n",
    "    blog_tokens = WhitespaceTokenizer().tokenize(buf.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Токенов:  12684\n",
      "Типов:  5511\n"
     ]
    }
   ],
   "source": [
    "print('Токенов: ', len(blog_tokens))\n",
    "print('Типов: ', len(set(blog_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "morph = MorphAnalyzer()\n",
    "freq = FreqDist(blog_tokens)\n",
    "blogs_parts_of_speech = {}\n",
    "for word in list(set(blog_tokens)):\n",
    "    # taking 4 first characters\n",
    "    part_of_speech = str(morph.parse(word)[0].tag)[:4] \n",
    "    if part_of_speech in blogs_parts_of_speech.keys():\n",
    "        blogs_parts_of_speech[part_of_speech] += int(freq[word])\n",
    "    else:\n",
    "        blogs_parts_of_speech[part_of_speech] = int(freq[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ADJF': 1599,\n",
       " 'ADJS': 127,\n",
       " 'ADVB': 842,\n",
       " 'COMP': 63,\n",
       " 'CONJ': 1286,\n",
       " 'GRND': 40,\n",
       " 'INFN': 404,\n",
       " 'INTJ': 37,\n",
       " 'LATN': 143,\n",
       " 'NOUN': 3802,\n",
       " 'NPRO': 644,\n",
       " 'NUMR': 63,\n",
       " 'PRCL': 641,\n",
       " 'PRED': 74,\n",
       " 'PREP': 1411,\n",
       " 'PRTF': 84,\n",
       " 'PRTS': 94,\n",
       " 'ROMN': 7,\n",
       " 'UNKN': 33,\n",
       " 'VERB': 1290}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blogs_parts_of_speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 20\n"
     ]
    }
   ],
   "source": [
    "print(len(science_parts_of_speech), len(blogs_parts_of_speech)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Difference in dictionary sizes means that blogs don't comprise some part of speech. To calculate spearman correlation coefficient dictionaries should be of same sizes, thus we correct data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PNCT']\n"
     ]
    }
   ],
   "source": [
    "diff = list(set(science_parts_of_speech.keys()) - set(blogs_parts_of_speech.keys()))\n",
    "if diff:\n",
    "    print(diff)\n",
    "    blogs_parts_of_speech[diff[0]] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "science_freq = [y for x, y in sorted(science_parts_of_speech.items(), key=lambda x: x[0])]\n",
    "blogs_freq = [y for x, y in sorted(blogs_parts_of_speech.items(), key=lambda x: x[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpearmanrResult(correlation=0.83885640025990893, pvalue=2.025563223320523e-06)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spearmanr(science_freq, blogs_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Low p-value tells us that we reject the null hypothesis, which states that distributions of different parts of speech are the same in both writing styles.\n",
    "\n",
    "In conclusion, the alternative hypothesis that different writing styles have different lexical distribution is accepted. It means that we can probably say, that all other writing styles differ comparing with lexical distribution as well. Though we cannot say that for sure, that's another hypothesis that should be validated."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
